# NLP-project — Telugu Poets Chatbot

This repository contains a lightweight hybrid chatbot (rule-based + retrieval) focused on Telugu poets and their poems. The project includes:

- A preprocessing script to convert a raw dataset into a structured JSON dataset (`preprocessing_code.py`).
- A retrieval + rule-based chatbot engine (`chatbot_engine.py`) that builds a TF-IDF knowledge base.
- A CLI runner (`run_chatbot.py`) to interact with the bot and save chat logs.
- The processed dataset (`Final_Dataset_Generated.json`) and supporting KB files (`knowledge_base_documents.txt`, `knowledge_base_metadata.json`).

This README explains how to set up the environment, prepare/regenerate the dataset, and run the chatbot from start to finish.

## Quick start (recommended)

1. Create and activate a Python virtual environment (recommended):

```bash
python3 -m venv .venv
source .venv/bin/activate
# NLP-project — Telugu Poets Chatbot

This repository contains a lightweight hybrid chatbot (rule-based + retrieval) focused on Telugu poets and their poems. The project includes:

- A preprocessing script to convert a raw dataset into a structured JSON dataset (`preprocessing_code.py`).
- A retrieval + rule-based chatbot engine (`chatbot_engine.py`) that builds a TF-IDF knowledge base.
- A CLI runner (`run_chatbot.py`) to interact with the bot and save chat logs.
- The processed dataset (`Final_Dataset_Generated.json`) and supporting KB files (`knowledge_base_documents.txt`, `knowledge_base_metadata.json`).

This README explains how to set up the environment, prepare/regenerate the dataset, and run the chatbot from start to finish.

## Quick start (recommended)

1. Create and activate a Python virtual environment (recommended):

```bash
python3 -m venv .venv
source .venv/bin/activate
```

2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. If you already have `Final_Dataset_Generated.json` in the repository, start the chatbot:

```bash
python3 run_chatbot.py
```

4. To exit the chat, type `quit`. The conversation will be saved to `chat_history.log`.

## When to (re)generate the dataset

The repository contains `raw_dataset.json` and the preprocessing script `preprocessing_code.py`. If you edit or replace `raw_dataset.json`, re-generate `Final_Dataset_Generated.json` by running:

```bash
python3 preprocessing_code.py
```

This will read `raw_dataset.json` and write `Final_Dataset_Generated.json` (as configured at the bottom of the preprocessing script).

Notes:
- The preprocessing script expects the raw records to contain the field `poet_data_paragraph` and optional `poems_raw`. The script parses text using regex and writes a structured list of poets (see the script comments for details).

## Files and purpose

- `preprocessing_code.py` — parse raw entries and write `Final_Dataset_Generated.json`.
- `Final_Dataset_Generated.json` — processed dataset used by the chatbot engine (list of poet objects).
- `raw_dataset.json` — original/raw data (used by preprocessing).
- `chatbot_engine.py` — chatbot implementation. It loads the JSON dataset, builds a TF-IDF retrieval KB, and contains rule-based handlers (regex patterns) for specific intents.
- `run_chatbot.py` — CLI runner that instantiates `ChatbotEngine` and starts a REPL-like loop.
- `knowledge_base_documents.txt` & `knowledge_base_metadata.json` — generated by `ChatbotEngine` when building the KB. They are useful for debugging/inspecting the retrieval index.
- `chat_history.log` — chat history created by the runner.
- `index.html` — present in the repo; currently the project is primarily CLI-based. Use this if you plan to add a web UI.

## Dependencies

Required Python packages are listed in `requirements.txt`. At minimum this project uses:

- Python 3.8+ (3.9/3.10 recommended)
- scikit-learn (TF-IDF vectorizer and cosine similarity)

Install via `pip install -r requirements.txt` or install the main dependency directly:

```bash
pip install scikit-learn
```

## How the bot works (short)

- On start, `ChatbotEngine` loads `Final_Dataset_Generated.json` and builds two artifacts:
  - a list of short documents + answers (saved to `knowledge_base_documents.txt`), and
  - a metadata file (`knowledge_base_metadata.json`) linking question examples to answers.
- It vectorizes those documents with TF-IDF and uses cosine similarity for retrieval.
- A set of regex-based rule handlers are checked first; if none match, the retrieval mechanism is used.

## Example conversations

Run `python3 run_chatbot.py` and try queries like (Telugu examples):

- "నన్నయ భట్టారకుడు గురించి చెప్పు"
- "తిక్కన సోమయాజి రచనలు ఏవి?"
- "నన్నయ భట్టారకుడు పద్యం ఒకటి చెప్పు"

If you see `క్షమించండి, మీ ప్రశ్న నాకు అర్థం కాలేదు.`, rephrase the question or try a name present in the dataset.

## Regenerating KB files

If you change `Final_Dataset_Generated.json`, the KB files will be regenerated the next time you run `run_chatbot.py` (the `ChatbotEngine` saves `knowledge_base_documents.txt` and `knowledge_base_metadata.json`). If you want to force a rebuild without running the CLI, you can run a small Python snippet:

```bash
python3 - <<'PY'
from chatbot_engine import ChatbotEngine
ChatbotEngine('Final_Dataset_Generated.json')
PY
```

That creates/overwrites `knowledge_base_documents.txt` and `knowledge_base_metadata.json` in the working directory.

## Troubleshooting

- If importing `sklearn` fails, ensure you installed the packages into the active virtual environment. Example:

```bash
source .venv/bin/activate
pip install -r requirements.txt
```

- If the chatbot prints a data load error, check the JSON file encoding (must be UTF-8) and that `Final_Dataset_Generated.json` is valid JSON.

- If retrieval answers are poor, try increasing the number of n-grams in `TfidfVectorizer` or cleaning the dataset (remove noisy lines) and regenerate KB files.

## Next steps / Suggestions

- Add a `requirements.txt` (included) and optionally a `pyproject.toml` or `venv` instructions for CI.
- Add a lightweight web UI (Flask/FastAPI) to serve the bot via REST and use `index.html` as a starting point.
- Add unit tests for `preprocess_raw_data` (happy path + malformed entry).

---

If you want, I can:

1. Add a `requirements.txt` (I will add one if it's missing).
2. Update this README with any additional environment constraints you prefer (Docker, specific Python minor version, etc.).

Tell me if you want me to write a `requirements.txt` now and commit it to the repo.
